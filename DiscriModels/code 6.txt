# Prediction model to distinguish advanced TIRADS thyroid nodules based on demographic information
rm(list = ls())
library(caret)
library(gbm)
library(ROCR)
library(dplyr)
library(rsample)
library(pROC)

#======== Step1: data input ========#
setwd("C:/users/Administrator/Desktop/data")
merge <- read.csv("input1.csv")     
mergeclass <- read.csv("input2.csv") 
mergeclass$cohort <- as.factor(mergeclass$cohort) 

newdata5 <- read.csv("newdata1.csv") 
set.seed(12)
n = dim(newdata5)[1]
index = sample(n,round((0.7)*n))   
train = newdata5[index,]         
test =  newdata5[-index,]      

write.csv(train,"train.csv")
write.csv(test,"test.csv")
#======== Step1: Parameter adjustment ========#
# Training the optimal parameter
hyper_grid_1 <- expand.grid(
  shrinkage = c(0.1,0.2,0.3), 
  interaction.depth = c(1,2,3), 
  n.minobsinnode = c(1,2,3), 
  bag.fraction = c(0.6,0.7,0.8), 
  optimal_trees = 0,               
  min_Error = 0                     
)

nrow(hyper_grid_1)

for(i in 1:nrow(hyper_grid_1)) {
  set.seed(123)
  # train model
  gbm.tune <- gbm(
    formula = cohort ~ .,
    distribution = "bernoulli",
    data = train,
    n.trees = 500,
    interaction.depth = hyper_grid_1$interaction.depth[i],
    shrinkage = hyper_grid_1$shrinkage[i],
    n.minobsinnode = hyper_grid_1$n.minobsinnode[i],
    bag.fraction = hyper_grid_1$bag.fraction[i],
    train.fraction = 0.7, 
    n.cores = NULL, 
    verbose = FALSE
  )
  
  hyper_grid_1$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid_1$min_Error[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid_1 %>% dplyr::arrange(min_Error) %>% head(5)

#======== Step2: Final model and evaluation ========#
set.seed(123)
system.time(
  # training GBM model
  gbm.fit.final <- gbm(
    formula = cohort ~ .,
    distribution = "bernoulli",
    data = train,
    n.trees = 300,
    shrinkage = 0.1,
    interaction.depth = 1,
    n.minobsinnode = 3,
    bag.fraction = 0.6, 
    train.fraction = 1, 
    cv.folds = 10, 
    n.cores = NULL, 
    verbose = FALSE
  )
)

sqrt(min(gbm.fit.final$cv.error))

par(mar = c(5, 8, 1, 1))
c00 <-summary(
  gbm.fit.final, 
  cBars = 10, 
  plotit = TRUE, 
  method = relative.influence,
  las = 2
)
write.csv(c00,"relative influence.csv")

# ROC and AUC
best.iter <- gbm.perf(gbm.fit.final,method = "cv")
f.predict <- predict(gbm.fit.final,test,best.iter)
pred <- prediction(f.predict,test$cohort)


perf <- performance(pred,'tpr',"fpr") 
auc <- performance(pred,'auc')
auc <- unlist(slot(auc,"y.values")) # auc value
auc
rocCurve <- roc(response = test$cohort, predictor = f.predict)
qujian<-ci.auc(rocCurve)
# ROC curve
plot(perf,
     xlim = c(0,1), ylim = c(0,1), col = 'red',
     main = paste("ROC curve(","AUC = ",auc,")"),
     lwd = 2, cex.main = 1.3, cex.lab = 1.2, cex.axis =1.2, font =1.2)
abline(0,1)
write.csv(perf@x.values[[1]],"perf1.csv")
write.csv(perf@y.values[[1]],"perf2.csv")
write.csv(perf@alpha.values[[1]],"perf3.csv")
# ACC
f.predict1 <-ifelse(f.predict>=0,1,0)
table(f.predict1,test$cohort)
mean(f.predict1 == test$cohort)
write.csv(test,"test373.csv")
sigmoid = function(x,a=1){1/(1+exp(-a*x))}
f.predict=sigmoid(f.predict)
write.csv(f.predict,"f.predict_test373.csv")


#################step 3: Verification##############
yanzheng <- read.csv("yanzheng.csv")
Process <- preProcess(yanzheng,method = "range")
yanzheng <- predict(Process, yanzheng)
set.seed(123)
system.time(
  gbm.fit.final <- gbm(
    formula = cohort ~ .,
    distribution = "bernoulli",
    data = train,
    n.trees = 300,
    shrinkage = 0.1,
    interaction.depth = 1,
    n.minobsinnode = 3,
    bag.fraction = 0.6, 
    train.fraction = 1, 
    cv.folds = 10, 
    n.cores = NULL, 
    verbose = FALSE
  )
)
######

sqrt(min(gbm.fit.final$cv.error))

par(mar = c(5, 8, 1, 1))
c001 <-summary(
  gbm.fit.final, # gbm object
  cBars = 10, # the number of bars to draw. length(object$var.names)
  plotit = TRUE, # an indicator as to whether the plot is generated.defult TRUE.
  method = relative.influence, # The function used to compute the relative influence. 
  las = 2
)
write.csv(c001,"relative influence.csv")
# ROC and AUC
#============= Evaluation effect after training GBM model =====================#
best.iter <- gbm.perf(gbm.fit.final,method = "cv")
f.predict_YANZHENG <- predict(gbm.fit.final,yanzheng,best.iter)
pred_YANGZHENG <- prediction(f.predict_YANZHENG,yanzheng$cohort)
perf_YANGZHENG <- performance(pred_YANGZHENG,'tpr',"fpr")
# plot(perf,col = 'blue', lty =2)
auc <- performance(pred_YANGZHENG,'auc')
auc <- unlist(slot(auc,"y.values"))
rocCurve <- roc(response = yanzheng$cohort, predictor = f.predict_YANZHENG)
qujian1<-ci.auc(rocCurve)
# ROC curve
plot(perf,
     xlim = c(0,1), ylim = c(0,1), col = 'red',
     main = paste("ROC curve(","AUC = ",auc,")"),
     lwd = 2, cex.main = 1.3, cex.lab = 1.2, cex.axis =1.2, font =1.2)
abline(0,1)
write.csv(perf_YANGZHENG@x.values[[1]],"perf11.csv")
write.csv(perf_YANGZHENG@y.values[[1]],"perf21.csv")
write.csv(perf_YANGZHENG@alpha.values[[1]],"perf31.csv")
# ACC
f.predict_YANG <-ifelse(f.predict_YANZHENG>=0,1,0)
table(f.predict_YANG,yanzheng$cohort)
mean(f.predict_YANG == yanzheng$cohort)
sigmoid = function(x,a=1){1/(1+exp(-a*x))}
f.predict_YANGZHENG1=sigmoid(f.predict_YANZHENG)
write.csv(f.predict_YANGZHENG1,"f.predict_YANGZHENG1.csv")

#======== Step4 : Generalization ability of model ===========#
finaldata <- read.csv("yanzheng.csv")
Process <- preProcess(finaldata,method = "range")
finaldata <- predict(Process, finaldata)
yanzhenglable <- read.csv("yanzhenglable.csv")
finaldata <- cbind(finaldata,yanzhenglable)

printauc_10times <- function(finaldata,train){
  final_auc <- array();              
  final_predict <- array();          
  final_roc_x <- array();         
  final_roc_y <- array();            
  final_roc_alpha <- array();       
  auc_cofindence_lower <- array();     
  auc_cofindence_upper <- array();     
  for(i in 1:100){
    set.seed(i)
    n = dim(finaldata)[1]
    index = sample(n,round((0.68)*n)) 
    final_test = finaldata[index,]
    print_paste <- paste(i,"Validation set.csv")    
    write.csv(final_test,print_paste)   
    
    #============= gbm model =====================#
    print(paste("Running the",i,"time"))  
    
    set.seed(123)
    gbm.fit.final <- gbm(
      formula = cohort ~ .,
      distribution = "bernoulli",    
      data = train,
      n.trees = 50,      
      shrinkage = 0.1,
      interaction.depth = 1,
      n.minobsinnode = 3,
      bag.fraction = 0.6,         
      train.fraction = 1,         
      cv.folds = 10,                 
      n.cores = NULL,               
      verbose = FALSE             
    )
    
    #============= Evaluation effect after training GBM model=====================#
    best.iter <- gbm.perf(gbm.fit.final,method = "cv")
    f.predict <- predict(gbm.fit.final,final_test,best.iter)
    pred <- prediction(f.predict,final_test$cohort)
    perf <- performance(pred,'tpr',"fpr")
    auc <- performance(pred,'auc')
    auc <- unlist(slot(auc,"y.values"))   # auc
    rocCurve <- roc(response = final_test$cohort, predictor = f.predict)
    auc_cofindence <- ci.auc(rocCurve) 
    
    
    final_auc <- append(final_auc,auc)
    final_predict <- append(final_predict,f.predict)
    final_roc_x <- append(final_roc_x,perf@x.values)
    final_roc_y <- append(final_roc_y,perf@y.values)
    final_roc_alpha <- append(final_roc_alpha,perf@alpha.values)
    auc_cofindence_lower <- append(auc_cofindence_lower,auc_cofindence[1])
    auc_cofindence_upper <- append(auc_cofindence_upper,auc_cofindence[3])
  }
  final_auc_pre <- list(final_auc = final_auc,final_predict = final_predict,final_roc_x = final_roc_x, final_roc_y = final_roc_y, final_roc_alpha = final_roc_alpha,auc_cofindence_lower = auc_cofindence_lower, auc_cofindence_upper = auc_cofindence_upper)
  return(final_auc_pre)
}

final_auc_pre <- printauc_10times(finaldata,train)  

# Output the final 100 AUCs
final_auc <- final_auc_pre$final_auc
final_auc <- final_auc[2:length(final_auc)]  
final_auc 
write.csv(final_auc,"final 100 auc.csv")

# Output the final 10 * 200 f.predicts
final_pre <- final_auc_pre$final_predict
final_pre <- final_pre[2:length(final_pre)]
final_pre_matirx <- matrix(final_pre, byrow = T,nrow = 10)  
final_pre_matirx <- as.data.frame(final_pre_matirx)
write.csv(final_pre_matirx,"2000¸öpredict.csv")

# Output the final x.values from perf
final_roc_x <- final_auc_pre$final_roc_x
xlsxflie=paste(1:10,sep="")
for(i in 1:10){
  c=paste("rocx",xlsxflie[i],sep="")
  output <- paste(c,".txt",sep = "")
  sink(output)
  print(final_roc_x[[i+1]])
  sink()
}


# Output the final y.values from perf
final_roc_y <- final_auc_pre$final_roc_y
xlsxflie=paste(1:10,sep="")
for(i in 1:10){
  c=paste("rocy",xlsxflie[i],sep="")
  output <- paste(c,".txt",sep = "")
  sink(output)
  print(final_roc_y[[i+1]])
  sink()
}

# Output the final  alpha.values from perf
final_roc_alpha <- final_auc_pre$final_roc_alpha
xlsxflie=paste(1:10,sep="")
for(i in 1:10){
  c=paste("rocalpha",xlsxflie[i],sep="")
  output <- paste(c,".txt",sep = "")
  sink(output)
  print(final_roc_alpha[[i+1]])
  sink()
}







